# Derin Ã–ÄŸrenme KavramlarÄ±: Genel Uygulamalar vs. Bizim Projemiz

Bu dokÃ¼man, gÄ±da iÅŸleme ve doku analizi projelerinde kullanÄ±lan standart yÃ¶ntemler ile `meat_virtual_image_processing` projesindeki Ã¶zel tercihlerimizi karÅŸÄ±laÅŸtÄ±rÄ±r.

---

## 1. GÃ¶rÃ¼ntÃ¼ Yeniden BoyutlandÄ±rma (Resize)

| Ã–zellik | Genel GÄ±da Projeleri | Bizim Projemiz (Meat Processing) | Neden? |
| :--- | :--- | :--- | :--- |
| **Boyut** | 128x128 ile 512x512 arasÄ± | **224x224** | Et dokusundaki (texture) bozulmalarÄ± yakalamak iÃ§in en verimli denge noktasÄ±dÄ±r. |
| **HÄ±z** | 128x128 (Ã‡ok hÄ±zlÄ±) | Orta / Segmentasyon seviyesi | Ã‡ok kÃ¼Ã§Ã¼k boyutlar etteki mikro renk deÄŸiÅŸimlerini "bulanÄ±klaÅŸtÄ±rabilir". |
| **Model Uyumu** | Ã‡eÅŸitli | **MobileNetV2 StandartÄ±** | KullandÄ±ÄŸÄ±mÄ±z ana mimari bu boyutta eÄŸitildiÄŸi iÃ§in aÄŸÄ±rlÄ±klar tam uyum saÄŸlar. |

**Proje UygulamasÄ±:** [`src/data_utils.py`](file:///c:/Users/ahmet/OneDrive/MasaÃ¼stÃ¼/Projects/meat_virtual_image_processing/src/data_utils.py) iÃ§inde OpenCV `cv2.resize` ile sabitlenmiÅŸtir.

---

## 2. Ã–ÄŸrenme OranÄ± (Learning Rate) ve Epoch

| Kavram | Standart YaklaÅŸÄ±m | Bizim Projemiz | AvantajÄ± |
| :--- | :--- | :--- | :--- |
| **Learning Rate** | Genelde sabittir (0.001) | **Dinamik / Azalan (Adaptive)** | `ReduceLROnPlateau` ile model zorlandÄ±ÄŸÄ±nda hÄ±z keserek hedefi Ä±skalamaz. |
| **Epoch Stratejisi** | Sabit bir sayÄ± (Ã¶rn. 50) | **Otomatik DurdurmalÄ± (EarlyStopping)** | Model geliÅŸim gÃ¶stermediÄŸi an eÄŸitimi keser, zaman ve gÃ¼Ã§ tasarrufu saÄŸlar. |
| **Optimum Bulma** | Manuel Grafik Takibi | **Otomatik Geri YÃ¼kleme** | EÄŸitim dursa bile en iyi epoch'taki aÄŸÄ±rlÄ±klarÄ± bulur ve geri yÃ¼kler. |

### GerÃ§ek EÄŸitim SonuÃ§larÄ±mÄ±z & Optimum KanÄ±tÄ±:
YaptÄ±ÄŸÄ±mÄ±z analizler sonucunda elde ettiÄŸimiz en iyi verileri iÃ§eren Ã¶zel grafiÄŸimiz:

![En Ä°yi EÄŸitim GrafiÄŸi](file:///c:/Users/ahmet/OneDrive/MasaÃ¼stÃ¼/Projects/meat_virtual_image_processing/outputs/plots/training_history_best.png)

- **Tespit Edilen Optimum:** **21. Epoch** (En dÃ¼ÅŸÃ¼k Validation Loss: `0.01358`)
- **KanÄ±t:** 21. epoch'tan sonra turuncu Ã§izginin (validation) yÃ¼kselmeye baÅŸlamasÄ±, modelin "ezberleme" (overfitting) evresine girdiÄŸini kanÄ±tlar.

---

## 3. Aktivasyon FonksiyonlarÄ± (ReLU ve Sigmoid)

| Fonksiyon | Genel KullanÄ±m AlanÄ± | Bizim Projemizdeki RolÃ¼ | Neden Kritik? |
| :--- | :--- | :--- | :--- |
| **ReLU** | Gizli Katmanlar | `dense_1` katmanÄ±nda | GÄ±da dokusundaki karmaÅŸÄ±k desenleri hÄ±zlÄ± ve gÃ¼rÃ¼ltÃ¼sÃ¼z Ã¶ÄŸrenmeyi saÄŸlar. |
| **Sigmoid** | SÄ±nÄ±flandÄ±rma | **Ã‡Ä±kÄ±ÅŸ KatmanÄ±nda (Score)** | Ã‡Ä±ktÄ±yÄ± 0-1 arasÄ±na hapseder. Bu, "Tazelik Skoru" iÃ§in mÃ¼kemmel bir metriktir. |

**Neden Sigmoid?** DiÄŸer projelerde bazen "Softmax" (Kategorik) kullanÄ±lÄ±r. Ancak biz bir **derecelendirme** (0:Taze, 1:Bozuk) istediÄŸimiz iÃ§in Sigmoid kullanarak hassas bir skor elde ediyoruz.

---

> [!TIP]
> **Ã–zet KarÅŸÄ±laÅŸtÄ±rma:** Genel projeler sadece "Bu ne?" sorusuna yanÄ±t ararken (SÄ±nÄ±flandÄ±rma), bizim projemiz **"Ne kadar taze?"** sorusuna yanÄ±t arar (Regresyon). Bu yÃ¼zden Sigmoid Ã§Ä±kÄ±ÅŸÄ± ve 224x224 model uyumu projemizin bel kemiÄŸidir.

---

## ğŸš¨ Kritik GÃ¼ncelleme: Ã‡ifte Normalizasyon (Double-Normalization) ve Ã‡Ã¶zÃ¼mÃ¼

Modelin ilk versiyonlarÄ±nda taze etler iÃ§in bile "orta bozulmuÅŸ" (0.6 - 0.76) gibi yÃ¼ksek skorlar Ã¼rettiÄŸi gÃ¶zlemlenmiÅŸtir. YapÄ±lan derinlemesine incelemede bunun **Ã‡ifte Normalizasyon** hatasÄ±ndan kaynaklandÄ±ÄŸÄ± tespit edilmiÅŸtir.

### Sorun Neydi?
1. **Manuel Normalizasyon:** Kodda gÃ¶rÃ¼ntÃ¼ler yÃ¼klenirken manuel olarak 255'e bÃ¶lÃ¼nÃ¼yordu (0-1 aralÄ±ÄŸÄ±).
2. **Model Ä°Ã§i Normalizasyon:** KullanÄ±lan `MobileNetV2` mimarisi, kendi `preprocess_input` katmanÄ±yla bu veriyi tekrar iÅŸliyor ve [-1, 1] aralÄ±ÄŸÄ±na Ã§ekmeye Ã§alÄ±ÅŸÄ±yordu.

Zaten normalize edilmiÅŸ (0-1) bir verinin tekrar normalize edilmesi, modelin karakteristik Ã¶zelliklerini "ezmiÅŸ" ve ciddi bir tahmin sapmasÄ±na (bias) yol aÃ§mÄ±ÅŸtÄ±r.

### Ã‡Ã¶zÃ¼m
Manuel `/ 255.0` iÅŸlemi eÄŸitim ve tahmin pipeline'Ä±ndan kaldÄ±rÄ±lmÄ±ÅŸtÄ±r. Bu dÃ¼zeltme sonrasÄ± model tekrar eÄŸitilerek tahmin hassasiyeti normale dÃ¶ndÃ¼rÃ¼lmÃ¼ÅŸtÃ¼r.
